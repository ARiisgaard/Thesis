\chapter{Evaluation Methods}\label{EvalMethods}

In this chapter the methods for evaluating the product will be presented.


Due to the limitations mentioned in section \ref{Lim} user testing was not a possibility. This meant that it was necessary to find another way to evaluate the user experience. This was done using Google Lighthouse version 5.7. 
This chapter starts with an overview of the tool followed by an evaluation of which criteria are relevant for this project. The relevant criteria and their metrics are then expanded upon.

\section{Setup}


The tests have been performed in Google Chrome version 81. To ensure that tests were unaffected by outside influences all Chrome extensions were disabled during testing. The cache was also disabled, so that every that tests would not be affect by previous tests.


\section{Overview}
Google Lighthouse is an automated tool in Chrome DevTools, which is the developer tool included in Chrome. It can be used to check the quality of websites within the five categories: Best practice, performance, accessibility, search engine optimization and progressive web app. \citep{Lighthouse}

\textbf{Performance}\\
The performance audit is measuring the site performance and load speed. This is done by measuring the time needed for different stages of loading and when the user is able to interact with the site. \citep{LhPerformance}


\textbf{Accessibility}\\
The accessibility check is for ensuring all users can effectively access and navigate the webpage. 
The check is mainly focused on Accessible Rich Internet Applications, which is used by assistive technologies such as screen readers.\citep{ARIA} It is pointed out that accessibility is difficult to automatically test, so further manual testing is recommended. 

\citep{LhAccess}

\textbf{Best practice}\\
The best practices cover different types of website enhancements. There are checks to ensure that the website is fast, secure, and not using deprecated technologies, among others. \citep{LhBP}

\textbf{Search engine optimization}\\
The checks within the Search Engine Optimization (SEO) category evaluate how well the page is optimized for ranking by search engine. This optimization is achieved by ensuring that the page is readable search engines and that search engines can access the page. It also includes some measurements for being mobile friendly.
\citep{LhSEO}

\textbf{Progressive web app}\\
The Progressive Web App (PWA) audits are target towards a mobile audience. These includes having a fast and reliable experience on mobile networks and to which degree the website act in a similar fashion to a native mobile application.
\citep{LhPWA}

\subsection{Relevant evaluation criteria}
 
Not all the audit categories in Google Lighthouse are relevant for this particular project. As mentioned in section \ref{Target audience}  the tool is developed for local use on a computer. This means that it is not relevant how optimized a site is for the phone. Since it is used locally it can not be found by a search engine, which makes the SEO category irrelevant. The best practices for enhancing the websites speed are relevant, while the security measures are not, since the tool is not accessible to others on the internet. While accessibility is important it has not been prioritised for the development of this prototype. The main argument is that the tool is being developed for data scientist and not for everybody. It is presumed that the majority of people, who work daily with computers have the sensory capacity to easily do so. A high performance is vital to ensuring a responsive user experience, so the performance audit will also be included.

\section{Metrics for performance}\label{MetricsForPerformance}

\fxnote{Source: Return to this one later}


\textbf{First Contentful Paint}

The First Contentful Paint (FCP) is the time in seconds it takes before the browser to render the first parts of the website. 
\citep{FCP}


\textbf{Speed Index}

The Speed Index (SI) is a time measurement of content appearing visually during load. To calculate it the visual progression between each frame of the loading process is measured. This is then being processed using the Speedline module. The unit is seconds. \citep{PerSI}

The Speedline module calculates the SI by calculating an Interval Score between each frame and then summaries all the Interval Scores. The Interval Score can be calculated using the formula \ref{SIFormula}, where the Interval is the elapsed time since last frame and Completeness is the percentage of visual completeness. \citep{SIformula}
\begin{equation}
 IntervalScore = Interval * \left( 1.0 - \frac{Completeness}{100} \right)
\end{equation}\label{SIFormula}


\textbf{First Meaningful Paint}

The First Meaningful Paint (FMP) is the seconds from the initial loading of the page to the primary content is visible. Where primary content is referring to the largest layout change, which is visible without scrolling.

It should be noted that FMP is being replaced with Largest Contentful Paint (LCP) in the next release of Chrome Lighthouse. The reason for this is that FMP did not give consistent result and was difficult to standardize in all web browsers. \citep{FMP}
LCP is the seconds needed to render the single largest content element, which is visible without scrolling.\citep{LCP}

\textbf{Time to Interactive}

The Time to Interactive (TTI) is the seconds before a loaded website is completely interactive. This time is defined as after FCP when most of visible elements can be interacted with and respond within 50 milliseconds. 
\citep{TTI}

\textbf{First CPU Idle}

First CPU Idle is the seconds before a page becomes minimally interactive. Minimally interactive is defined as when the majority of the visible User Interface elements are interactive and giving responds within a reasonable time. 
The difference between this and TTI is the degree of interaction. When the user can begin interacting with a page the First CPU Idle can be measured. TTI is measured when all interactions can be performed. 
This feature is being replaced with Total Blocking Time (TBT) in the next Lighthouse release. The reason for this is that this and TTI are too similar to maintain both. 
\citep{FCPUI}
TBT is the total amount of milliseconds between FCP and TTI where the website is not responding to user input.
\citep{TBT}

\section{Calculating score}\label{ScoreCal}

Each of the metrics in the section above are then converted into a score between 0 and 100. This score is calculated by comparing each metric with performance data from real websites in the HTTP Archive.
https://web.dev/performance-scoring/
The scores from each metrics are then weighted to reflect that each factor does not have the same importance for the perceived performance. The weight of each metric can be seen in table x.

\fxnote{Make table with score weights}

\section{Additional measurements}

The measurement mentioned so far only take the initial load into consideration, not the postload performance. 
Therefore, two additional measurements were conducted. Both of these were measured using the Performance tool in the Google Chrome Developer tool. Since there are slight variation in the time needed to perform the requests, each operation was tested multiple times and the average was calculated. 
\subsection{Time to color new layer}
This is the time in seconds required to load and color completely new sets of tiles for both maps. The scenario is created by zooming to a new zoom extent. This is done to ensure that none of the tiles on the map have been loaded before. The measured time is the time from the map is clicked, initiating the zooming, till the layer have been colored.

\subsection{Time to recolor a loaded layer}
This measurement is the time to recolor a layer, which already have been loaded. This scenario is created by panning to a previously loaded part of the map, so that it is recolored. 
It is measured from the moment, the panning stops since the recoloring script is initiated on ended movement. The timer is stopped, when the layer is recolored.

