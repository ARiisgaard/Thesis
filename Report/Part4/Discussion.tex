\section{Discussion}
\chapter{Discussion}
\section{Further development}
Not all the intended functionality got added to the map. This section will list some of the features, which there was not time to implement. 

\subsection{Data at point of click}

A function could be added to get the population counts from a clicked point for both maps. An example of this have been shown in figure x. When the user clicks the map an infobox informs the user about the value at the clicked point for both maps. 
TODO: Figure showing a popup on click
-	Popup text: Data in this map and data in the other



\subsection{Parallel generation of tiles}
As mentioned in section x the generation of tiles were done without using multiple processers, which meant that it became a time-consuming process. With the official gdal2tiles being able to generate tiles following the XYZ standard it is possible that using this would allow for parallel generation of tiles. 
This is probably the most important missing feature, since the processing time otherwise would be so high, that it would not be a faster alternative to the currently available options. 

\subsection{Changing layers}
Another functionality, which could have been added was the option to change the datasets of the maps. As mentioned in section x the case data contains population projection for ten different years. It does therefore make sense to have the option display more than the two datasets.  As illustrated in figure x data from other years could be added as separate layers. This would allow the user to compare different datasets without having to reload the webgis with different datasets.

This way the map could also be used to visualize the different in the same projection from one year and another.
\subsection{Option for multiple colorschemes}
If there is a vast difference between the values in the two shown projection coloring based on the same maximum values does not necessarily make sense as was shown in the figure in core concept 2. It would therefore make sense to have the option to enable separate coloring as shown in figure x. 
TODO: make a figure showing the map with multiple legends and color schemes. 


\section{Cloud Optimized Geotiff}

An alternative to separating the rasterfile into smaller tiles would be to have the entire raster as one file, but then only send part of the file. This is possible using Cloud Optimized GeoTIFF (COG). These are GeoTiff files, which are organized in internal “tiles”.
The files also contain multiple versions of the same image, where each is a downsampled version of the original. Each of these image versions can match a different zoom level.


This way of organizing the file is then combined with Range requests, which allows the client to request parts of a file instead of the whole file.
https://www.cogeo.org/in-depth.html

By using these two technologies tiles of different spatial resolution can be sent to the client, which is similar to what the current solution is doing. The main difference is that with COG there would be one file, while the current solution has thousands of files. The creation of this file takes significantly less time compared to creating the individual tiles. India was as mentioned in section x processed into tiles in 6 hours and four minutes. Converting the same file into a COG with the command in equation x takes 3 seconds.
gdal_translate in.tif out.tif -co TILED=YES -co COPY_SRC_OVERVIEWS=YES 
https://www.cogeo.org/developers-guide.html
This way parts of only the parts of the raster, which the user can see on the map get requested.


There is currently no support for COG in Openlayers
https://github.com/openlayers/openlayers/issues/10733
and no extensions enabling it
https://github.com/cogeotiff/www.cogeo.org/issues/36
, but the is support for visualizing the files in Leaflet
https://github.com/GeoTIFF/georaster-layer-for-leaflet


\section{Performance enhancements}

Based on the performance and best practice audits by Google Lighthouse the performance could be enhanced in multiple ways. 

\textbf{Eliminate render-blocking resources}
As mentioned in section x the largest render-blocking resource was Openlayers. The same section also highlighted that 43.2 \% of the library never get used. The performance could therefore be improved by only loading the necessary parts of the library. 
\fxnote{Check with Carsten – can this only be done with node?}
\citep{OlModule}

\textbf{Avoid enormous network payloads}
The size of data loads will be reduced by only having to load one set of tiles with the solution mentioned in section x. 

\fxnote{Write about asynchronous XMLREquest and minifying/compressing}
-	Tenser 

\fxnote{Write about zoom level bug}
\fxnote{Add user testing to future work }

\textbf{Does not use HTTP/2 for all of its resources}
Caddy was used to serve the resources with HTTP/2. However even after setting up Caddy the error message was still present. It is uncertain if this means that the connection still is HTTP/1 based or if it is HTTP/2 but labelled incorrectly. Figure x is two screenshots from the network tab of chromes developer tool when the page is served with a python server (top) and caddy server (bottom). 
The grey bars are when a loaded file is being stalled. The blue bars are when they are loaded. This figure shows that the serving is being optimized, where the stalling largely is gone. This could indicate that the server no longer has the HTTP/1 limit of only having 6 TCP connections.
 \fxnote{Source!!!}

\begin{figure} [H]
	\centering
	\includegraphics[width=.8\textwidth]{Pictures/CaddyVsPython}
	\caption{Screenshot of the network connection for a python and a caddy server}
	\label{CaddyVsPython}
\end{figure}


\fxnote{Mention caddy – 6 connection issue earlier}
 

\section{Not calculating minimum values in tiles} \label{WhyNoMin}

In the original version of the map both the minimum and maximum values were being calculated. However the program consistently defined the lowest value in currently visible tiles as 0.
This does not mean that all the tiles had minimum values of 0, just that there always were a tile in the map extent with a value of 0. In the vast majority of case the value would be 0. Few would be below 100. In the densest of cities the minimum value for some tiles was higher than 1000. 
It is possible that by zoom close enough to a major city it none of the tiles within view would have a value of 0 in which case the minimum value should be adjusted.
This is the main argument for calculating a minimum value. The downside of calculating it is that it has an significant effect on the processing time of the raster layer. Table \ref{tabMinimum} shows the time in seconds it takes for the layer to be rendered with and without calculating the minimum value. On average the rendering time is a second faster, when not calculating the minimum value.

\begin{table}[htbp]
	\centering
	\begin{tabular}{l}
		\includegraphics[width=0.8\textwidth]{Pictures/tabMinimum}
	\end{tabular}
	\caption{Seconds to fully render the website with and without calculating the minimum value}
	\label{tabMinimum}
\end{table}

The reason that the calculation of minimum value has such an effect on the processing time is presumably that the minimum value cannot be calculated in the same way as the maximum value. 
The maximum value is calculated by finding the maximum value in the array with all values. 
\fxnote{Write about nodata}
If the same operation is done to find the minimum value, the found minimum value often is -2147483648. As mentioned in section x that value means that no data is available for that pixel. 
It is therefore necessary to find the lowest positive value since no data is not the same as 0. This can be done by first filtering the raster, so only the positive values are left. The smallest of these values can then be calculated in the same way as the maximum value get calculated. 
It was decided that calculating a minimum value, which mostly would be 0 was not worth an additional second of loading. 

\section{Is this even a good measurement for performance?}
